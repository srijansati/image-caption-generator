{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb02649b-6412-4b8c-9e11-e6f194581744",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.applications.vgg16 import VGG16\n",
    "import keras.utils as image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "778c35ce-c2eb-4720-938b-8c9217729475",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m## **Feature extraction using VGG16** \u001b[39;00m\n\u001b[0;32m      2\u001b[0m directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mimage-caption-generator\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mFlickr8k_Dataset\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 3\u001b[0m VGG_model \u001b[38;5;241m=\u001b[39m \u001b[43mVGG16\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m VGG_model \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mModel(inputs \u001b[38;5;241m=\u001b[39m VGG_model\u001b[38;5;241m.\u001b[39minputs, outputs \u001b[38;5;241m=\u001b[39m VGG_model\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39moutput)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(VGG_model\u001b[38;5;241m.\u001b[39msummary())\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\applications\\vgg16.py:235\u001b[0m, in \u001b[0;36mVGG16\u001b[1;34m(include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation)\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimagenet\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m include_top:\n\u001b[1;32m--> 235\u001b[0m         weights_path \u001b[38;5;241m=\u001b[39m \u001b[43mdata_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvgg16_weights_tf_dim_ordering_tf_kernels.h5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m            \u001b[49m\u001b[43mWEIGHTS_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_subdir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfile_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m64373286793e3c8b2b4e3219cbf3544b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    242\u001b[0m         weights_path \u001b[38;5;241m=\u001b[39m data_utils\u001b[38;5;241m.\u001b[39mget_file(\n\u001b[0;32m    243\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    244\u001b[0m             WEIGHTS_PATH_NO_TOP,\n\u001b[0;32m    245\u001b[0m             cache_subdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    246\u001b[0m             file_hash\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m6d6bbae143d832006294945121d1f1fc\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    247\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\data_utils.py:259\u001b[0m, in \u001b[0;36mget_file\u001b[1;34m(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(fpath):\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;66;03m# File found; verify integrity if a hash was provided.\u001b[39;00m\n\u001b[0;32m    258\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_hash \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 259\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mvalidate_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_hash\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malgorithm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhash_algorithm\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    260\u001b[0m             io_utils\u001b[38;5;241m.\u001b[39mprint_msg(\n\u001b[0;32m    261\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA local file was found, but it seems to be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    262\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincomplete or outdated because the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhash_algorithm\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    265\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mso we will re-download the data.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    266\u001b[0m             )\n\u001b[0;32m    267\u001b[0m             download \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\data_utils.py:392\u001b[0m, in \u001b[0;36mvalidate_file\u001b[1;34m(fpath, file_hash, algorithm, chunk_size)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Validates a file against a sha256 or md5 hash.\u001b[39;00m\n\u001b[0;32m    378\u001b[0m \n\u001b[0;32m    379\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;124;03m    Whether the file is valid\u001b[39;00m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    390\u001b[0m hasher \u001b[38;5;241m=\u001b[39m _resolve_hasher(algorithm, file_hash)\n\u001b[1;32m--> 392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(\u001b[43m_hash_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhasher\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mstr\u001b[39m(file_hash):\n\u001b[0;32m    393\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\data_utils.py:370\u001b[0m, in \u001b[0;36m_hash_file\u001b[1;34m(fpath, algorithm, chunk_size)\u001b[0m\n\u001b[0;32m    367\u001b[0m     hasher \u001b[38;5;241m=\u001b[39m algorithm\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(fpath, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fpath_file:\n\u001b[1;32m--> 370\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m: fpath_file\u001b[38;5;241m.\u001b[39mread(chunk_size), \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    371\u001b[0m         hasher\u001b[38;5;241m.\u001b[39mupdate(chunk)\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hasher\u001b[38;5;241m.\u001b[39mhexdigest()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\data_utils.py:370\u001b[0m, in \u001b[0;36m_hash_file.<locals>.<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    367\u001b[0m     hasher \u001b[38;5;241m=\u001b[39m algorithm\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(fpath, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fpath_file:\n\u001b[1;32m--> 370\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[43mfpath_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    371\u001b[0m         hasher\u001b[38;5;241m.\u001b[39mupdate(chunk)\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hasher\u001b[38;5;241m.\u001b[39mhexdigest()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## **Feature extraction using VGG16** \n",
    "directory = 'F:\\image-caption-generator\\Flickr8k_Dataset'\n",
    "VGG_model = VGG16()\n",
    "VGG_model = keras.Model(inputs = VGG_model.inputs, outputs = VGG_model.layers[-2].output)\n",
    "print(VGG_model.summary())\n",
    "\n",
    "feature = dict()\n",
    "\n",
    "for i in tqdm(os.listdir(directory)):\n",
    "    file_name = directory + '/' + i\n",
    "    img = image.load_img(file_name, target_size = (224, 224))\n",
    "    img = image.img_to_array(img)\n",
    "    img = img.reshape((1,224,224,3))\n",
    "    img = preprocess_input(img)\n",
    "\n",
    "    features = VGG_model.predict(img)\n",
    "    image_id = i.split('.')[0]\n",
    "\n",
    "    feature[image_id] = features\n",
    "\n",
    "print(\"Extracted Features: %d\" % len(feature))\n",
    "pickle.dump(feature, open('features.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f203b37-6cb0-4494-b554-633bf7e99742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Captions 8092\n",
      "Vocabulary Size 8763\n"
     ]
    }
   ],
   "source": [
    "filename = \"F:\\\\image-caption-generator\\\\Flickr8k_text\\\\Flickr8k.token.txt\"\n",
    "\n",
    "# Loading the caption data\n",
    "file = open(filename, 'r')\n",
    "caption = file.read()\n",
    "file.close()\n",
    "\n",
    "# Extracting the caption for image data\n",
    "descriptions = dict()\n",
    "for line in caption.split('\\n'):\n",
    "    tokens = line.split()\n",
    "\n",
    "    if len(line) < 2:\n",
    "        continue\n",
    "\n",
    "    image_id, image_description = tokens[0], tokens[1:]\n",
    "    image_id = image_id.split('.')[0]\n",
    "    image_description = ' '.join(image_description)\n",
    "\n",
    "    if image_id not in descriptions:\n",
    "        descriptions[image_id] = list()\n",
    "\n",
    "    descriptions[image_id].append(image_description)\n",
    "\n",
    "print(\"Loaded Captions\", len(descriptions))\n",
    "\n",
    "# Cleaning the extracted caption descriptions of the image data\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "for key, desc_list in descriptions.items():\n",
    "    for i in range(len(desc_list)):\n",
    "        description = desc_list[i]\n",
    "\n",
    "        # Tokenize\n",
    "        description = description.split()\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        description = [term.lower() for term in description]\n",
    "\n",
    "        # Remove punctuation from text\n",
    "        description = [w.translate(table) for w in description]\n",
    "\n",
    "        # Remove single-character words from captions\n",
    "        description = [term for term in description if len(term) > 1]\n",
    "\n",
    "        # Remove tokens with numbers\n",
    "        description = [term for term in description if term.isalpha()]\n",
    "\n",
    "        # Store the result as a string\n",
    "        desc_list[i] = ' '.join(description)\n",
    "\n",
    "# Creating vocabulary\n",
    "all_desc = set()\n",
    "for key in descriptions.keys():\n",
    "    [all_desc.update(d.split()) for d in descriptions[key]]\n",
    "\n",
    "print('Vocabulary Size', len(all_desc))\n",
    "\n",
    "# Save to file\n",
    "lines = []\n",
    "for key, desc_list in descriptions.items():\n",
    "    for description in desc_list:\n",
    "        lines.append(key + ' ' + description)\n",
    "data = '\\n'.join(lines)\n",
    "\n",
    "file = open('descriptions.txt', 'w')\n",
    "file.write(data)\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f85cb2f8-6275-4780-b569-644c8722e67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(file_name):\n",
    "    file = open(file_name, 'r')\n",
    "    caption = file.read()\n",
    "    file.close()\n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d38e91e-be2d-41d2-9ee6-190c0d43b7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_set(file_name):\n",
    "    document = load_doc(file_name)\n",
    "    data_set = list()\n",
    "    \n",
    "    # process line by line\n",
    "    for line in document.split('\\n'):\n",
    "        # skip empty lines\n",
    "        if len(line) < 1:\n",
    "            continue\n",
    "        \n",
    "        # get the image identifier\n",
    "        identifier = line.split('.')[0]\n",
    "        data_set.append(identifier)\n",
    "\n",
    "    return set(data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f237ece2-f4fc-400e-b072-1ffe1ea4eec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load clean descriptions into memory\n",
    "def load_clean_descriptions(file_name, data_set):\n",
    "    # load document\n",
    "    doc = load_doc(file_name)\n",
    "#     print(doc)\n",
    "    desc = dict()\n",
    "#     print(doc.split('\\n'))\n",
    "    for line in doc.split('\\n'):\n",
    "        # split line by white space\n",
    "        tokens = line.split()\n",
    "        \n",
    "        # split id from description\n",
    "        image_id, image_desc = tokens[0], tokens[1:]\n",
    "        if image_id in data_set:\n",
    "            # create list\n",
    "            if image_id not in list(desc.keys()):\n",
    "                desc[image_id] = list()\n",
    "                \n",
    "            # wrap desc in tokens\n",
    "            description = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "            \n",
    "            # store\n",
    "            desc[image_id].append(description)\n",
    "    return desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11ca8022-6057-4225-94a4-7970f618dc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load photo features\n",
    "def load_photo_features(file_name, data_set):\n",
    "    # load all features\n",
    "    all_features = pickle.load(open(file_name, 'rb'))\n",
    "    \n",
    "    # filter features\n",
    "    features = {k: all_features[k] for k in data_set}\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95b567af-ebae-44a0-aae3-07fa50a789fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lines(desc):\n",
    "    all_descriptions = list()\n",
    "    for key in desc.keys():\n",
    "        list(all_descriptions.append(d) for d in desc[key])\n",
    "    return all_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04a2c745-08ee-429b-afd2-6459055924ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(desc):\n",
    "    lines = to_lines(desc)\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74bf9b9b-7a43-4f4d-800f-c3b66dd7b765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(desc):\n",
    "    lines = to_lines(desc)\n",
    "    return max(len(d.split()) for d in lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bc64c0f-8039-4877-8099-f3396f08f8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 6000\n",
      "Descriptions: train= 6000\n",
      "Photos: train= 6000\n",
      "Vocabulary Size: 7579\n",
      "Description Length: 34\n"
     ]
    }
   ],
   "source": [
    "filename = r\"F:\\image-caption-generator\\Flickr8k_text\\Flickr_8k.trainImages.txt\"\n",
    "\n",
    "train = load_set(filename)\n",
    "print(\"Dataset:\",len(train))\n",
    "\n",
    "# descriptions\n",
    "train_descriptions = load_clean_descriptions(r\"F:\\image-caption-generator\\workspace\\descriptions.txt\", train)\n",
    "print(\"Descriptions: train=\",len(train_descriptions))\n",
    "\n",
    "# photo features\n",
    "train_features = load_photo_features(r\"F:\\image-caption-generator\\workspace\\features.pkl\", train)\n",
    "print(\"Photos: train=\",len(train_features))\n",
    "\n",
    "# prepare tokenizer\n",
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\"Vocabulary Size:\",vocab_size)\n",
    "\n",
    "# determine the maximum sequence length\n",
    "max_length = max_length(train_descriptions)\n",
    "print(\"Description Length:\",max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aeac24f3-c7bc-484c-b7a9-27b7ce2e5ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# create sequences of images, input sequences and output words for an image\n",
    "def create_sequences(tokenizer, max_length, desc_list, photo):\n",
    "    x_1 = []\n",
    "    x_2 = []\n",
    "    y = []\n",
    "    \n",
    "    for description in desc_list:\n",
    "        # encode the sequence\n",
    "        sequence = tokenizer.texts_to_sequences([description])[0]\n",
    "        \n",
    "        # splitting one seqeunce into multiple x and y pairs\n",
    "        for i in range(1, len(sequence)):\n",
    "            # split in input and output pair\n",
    "            input_seq, output_seq = sequence[:i], sequence[i]\n",
    "            \n",
    "            # pad input sequence\n",
    "            in_seq = pad_sequences([input_seq], maxlen=max_length)[0]\n",
    "            \n",
    "            # encode output sequence\n",
    "            out_seq = to_categorical([output_seq], num_classes=vocab_size)[0]\n",
    "            \n",
    "            # store\n",
    "            x_1.append(photo)\n",
    "            x_2.append(input_seq)\n",
    "            y.append(output_seq)\n",
    "            \n",
    "    x_1 = np.array(x_1,object)\n",
    "    x_2 = np.array(x_2, object)\n",
    "    y = np.array(y, object)\n",
    "    print(x_1.shape)\n",
    "    print(x_2.shape)\n",
    "    print(y.shape)\n",
    "    return x_1, x_2, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9237aa74-b0d4-4b41-a12b-88d6ed95e217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below code is used to progressively load the batch of data\n",
    "# data generator will be used in the model.fit_generator()\n",
    "def data_generator(desc, photos, tokenizer, max_length, batch_size):\n",
    "    # loop for ever over images\n",
    "    x_1, x_2, y = list(), list(), list()\n",
    "    n = 0\n",
    "    while True:\n",
    "        for key, desc_list in desc.items():\n",
    "            n+=1\n",
    "            # retrieving the photo features\n",
    "            photo = photos[key][0]\n",
    "            \n",
    "            for description in desc_list:\n",
    "                # encode the sequence\n",
    "                sequence = tokenizer.texts_to_sequences([description])[0]\n",
    "\n",
    "                # splitting one seqeunce into multiple x and y pairs\n",
    "                for i in range(1, len(sequence)):\n",
    "                    # split in input and output pair\n",
    "                    input_seq, output_seq = sequence[:i], sequence[i]\n",
    "\n",
    "                    # pad input sequence\n",
    "                    in_seq = pad_sequences([input_seq], maxlen=max_length)[0]\n",
    "\n",
    "                    # encode output sequence\n",
    "                    out_seq = to_categorical([output_seq], num_classes=vocab_size)[0]\n",
    "\n",
    "                    # store\n",
    "                    x_1.append(photo)\n",
    "                    x_2.append(in_seq)\n",
    "                    y.append(out_seq)\n",
    "\n",
    "            if n == batch_size:\n",
    "                x_1, x_2, y = np.array(x_1), np.array(x_2), np.array(y)\n",
    "                yield [x_1, x_2], y\n",
    "                x_1, x_2, y = list(), list(), list()\n",
    "                n = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73e4f10b-8a20-4246-b075-213a96b6694e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the captioning model\n",
    "def define_model(vocab_size, max_length):\n",
    "    # feature extractor model\n",
    "    inputs1 = keras.Input(shape=(4096,))\n",
    "    fe_1 = layers.Dropout(0.5)(inputs1)\n",
    "    fe_2 = layers.Dense(256, activation=tf.nn.relu)(fe_1)\n",
    "    \n",
    "    # The sequence model takes input sentences or descriptions to be fed to the embedding layer\n",
    "    # sequence model\n",
    "    inputs2 = layers.Input(shape=(max_length,))\n",
    "    \n",
    "    # the parameter mask_true is set as true to ignore the padded values\n",
    "    # The input sequences are of length 34 words\n",
    "    se_1 = layers.Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "    \n",
    "    # using a dropout layer to reduce overfitting\n",
    "    se_2 = layers.Dropout(0.5)(se_1)\n",
    "        \n",
    "    # After this, we will use an LSTM layer having 256 memory units \n",
    "    # to process these text descriptions of the sentences.\n",
    "    se_3 = layers.LSTM(256)(se_2)\n",
    "    \n",
    "    # decodeer model\n",
    "    # the decoder model merges the vectors from both the \n",
    "    # input models by doing an addition operation.\n",
    "    decoder_1 = layers.add([fe_2, se_3])\n",
    "    decoder_2 = layers.Dense(256, activation=tf.nn.relu)(decoder_1)\n",
    "    outputs = layers.Dense(vocab_size, activation=tf.nn.softmax)(decoder_2)\n",
    "    \n",
    "    # tie it together [image, sequence] [word]\n",
    "    model = keras.Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(loss=keras.losses.CategoricalCrossentropy(), optimizer=keras.optimizers.Adam())\n",
    "    \n",
    "    # summarize mode\n",
    "    print(model.summary())\n",
    "    \n",
    "    plot_model(model, show_shapes=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a5a1bbe-a7d4-46b2-b0db-93564ddb997c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 34)]         0           []                               \n",
      "                                                                                                  \n",
      " input_1 (InputLayer)           [(None, 4096)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 34, 256)      1940224     ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 4096)         0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 34, 256)      0           ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 256)          1048832     ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, 256)          525312      ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 256)          0           ['dense[0][0]',                  \n",
      "                                                                  'lstm[0][0]']                   \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 256)          65792       ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 7579)         1947803     ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,527,963\n",
      "Trainable params: 5,527,963\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\srija\\AppData\\Local\\Temp\\ipykernel_11956\\1453134953.py:14: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2/93 [..............................] - ETA: 51s - loss: 8.7075  "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m generator \u001b[38;5;241m=\u001b[39m data_generator(train_descriptions, train_features, tokenizer, max_length, batch_size)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# fit for one epoch\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# save the model\u001b[39;00m\n\u001b[0;32m     17\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(i) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py:2507\u001b[0m, in \u001b[0;36mModel.fit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2495\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fits the model on data yielded batch-by-batch by a Python generator.\u001b[39;00m\n\u001b[0;32m   2496\u001b[0m \n\u001b[0;32m   2497\u001b[0m \u001b[38;5;124;03mDEPRECATED:\u001b[39;00m\n\u001b[0;32m   2498\u001b[0m \u001b[38;5;124;03m  `Model.fit` now supports generators, so there is no longer any need to\u001b[39;00m\n\u001b[0;32m   2499\u001b[0m \u001b[38;5;124;03m  use this endpoint.\u001b[39;00m\n\u001b[0;32m   2500\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2501\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   2502\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`Model.fit_generator` is deprecated and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2503\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill be removed in a future version. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2504\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease use `Model.fit`, which supports generators.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2505\u001b[0m     stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m   2506\u001b[0m )\n\u001b[1;32m-> 2507\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2509\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2510\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2511\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2512\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2513\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2514\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2515\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2518\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2519\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2520\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2521\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2522\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# train the model\n",
    "model = define_model(vocab_size, max_length)\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "steps = len(train_descriptions) // batch_size\n",
    "\n",
    "for i in range(epochs):\n",
    "    # create the data generator\n",
    "    generator = data_generator(train_descriptions, train_features, tokenizer, max_length, batch_size)\n",
    "    \n",
    "    # fit for one epoch\n",
    "    model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "    \n",
    "    # save the model\n",
    "    model.save('model_' + str(i) + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bad728d0-e808-4d43-9829-6138b03e50f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\srija\\AppData\\Local\\Temp\\ipykernel_7796\\605340761.py:12: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 41s 406ms/step - loss: 1.0021\n",
      "93/93 [==============================] - 39s 415ms/step - loss: 0.9983\n",
      "93/93 [==============================] - 39s 417ms/step - loss: 0.9981\n",
      "93/93 [==============================] - 39s 417ms/step - loss: 0.9968\n",
      "93/93 [==============================] - 39s 418ms/step - loss: 0.9953\n",
      "93/93 [==============================] - 39s 419ms/step - loss: 0.9953\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "model = tf.keras.models.load_model('model_494.h5')\n",
    "epochs = 6\n",
    "batch_size = 64\n",
    "steps = len(train_descriptions) // batch_size\n",
    "\n",
    "for i in range(epochs):\n",
    "    # create the data generator\n",
    "    generator = data_generator(train_descriptions, train_features, tokenizer, max_length, batch_size)\n",
    "    \n",
    "    # fit for one epoch\n",
    "    model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "    \n",
    "    # save the model\n",
    "    model.save('model_' + str(i + 495) + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a52a2862-0be2-4eae-9698-3d232d5447ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02b87da1-352e-452a-ba36-b416490f907a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_desc(model, tokenizer, photo, max_length):\n",
    "    # seed the generation process\n",
    "    in_text = \"startseq\"\n",
    "    for i in range(max_length):\n",
    "        # integer encode input sequence\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        \n",
    "        # pad input\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        \n",
    "        # predict next word\n",
    "        yhat = model.predict([photo, sequence], verbose=0)\n",
    "        \n",
    "        # convert proba to integer\n",
    "        yhat = np.argmax(yhat)\n",
    "        \n",
    "        # map integer to word\n",
    "        word = word_for_id(yhat, tokenizer)\n",
    "        \n",
    "        if word is None:\n",
    "            break\n",
    "           \n",
    "        # append as input for generating the next word\n",
    "        in_text += ' ' + word\n",
    "        \n",
    "        # we will stop if we predict the endseq\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "            \n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "faed6506-efc7-48be-9ea9-718d0688e77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# evalute the skill of the mode\n",
    "def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n",
    "    actual, predicted = list(), list()\n",
    "    # step over the whole set\n",
    "    for key, desc_list in tqdm(descriptions.items()):\n",
    "        # generate description\n",
    "        yhat = generate_desc(model, tokenizer, photos[key], max_length)\n",
    "        \n",
    "        # store actual and predicted\n",
    "        references = [d.split() for d in desc_list]\n",
    "        actual.append(references)\n",
    "        predicted.append(yhat.split())\n",
    "        \n",
    "    # calculate BLEU score\n",
    "    print(\"BLEU-1: %f\" % corpus_bleu(actual, predicted, weights=(1.0, 0,0, 0)))\n",
    "    print(\"BLEU-2: %f\" % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "    print(\"BLEU-3: %f\" % corpus_bleu(actual, predicted, weights=(0.256, 0.25, 0.25, 0.25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d62e7e7-d33e-4ac8-a24d-ef9fbb63536d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 1000\n",
      "Descriptions: test=1000\n",
      "Photos: test=1000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# load the model which has minimum loss, in this case it was model_18\u001b[39;00m\n\u001b[0;32m     14\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mimage-caption-generator\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mworkspace\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmodel_500.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 15\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# evalute the model\u001b[39;00m\n\u001b[0;32m     18\u001b[0m evaluate_model(model, test_descriptions, test_features, tokenizer, max_length)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\saving\\save.py:241\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m h5py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    236\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m    237\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFilepath looks like a hdf5 file but h5py is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    238\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot available.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    239\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    240\u001b[0m             )\n\u001b[1;32m--> 241\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhdf5_format\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model_from_hdf5\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m h5py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filepath, h5py\u001b[38;5;241m.\u001b[39mFile):\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hdf5_format\u001b[38;5;241m.\u001b[39mload_model_from_hdf5(\n\u001b[0;32m    248\u001b[0m         filepath, custom_objects, \u001b[38;5;28mcompile\u001b[39m\n\u001b[0;32m    249\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\saving\\hdf5_format.py:200\u001b[0m, in \u001b[0;36mload_model_from_hdf5\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    198\u001b[0m     model_config \u001b[38;5;241m=\u001b[39m model_config\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    199\u001b[0m model_config \u001b[38;5;241m=\u001b[39m json_utils\u001b[38;5;241m.\u001b[39mdecode(model_config)\n\u001b[1;32m--> 200\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_config_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_from_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;66;03m# set weights\u001b[39;00m\n\u001b[0;32m    205\u001b[0m load_weights_from_hdf5_group(f[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m], model)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\saving\\model_config.py:55\u001b[0m, in \u001b[0;36mmodel_from_config\u001b[1;34m(config, custom_objects)\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m     49\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`model_from_config` expects a dictionary, not a list. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     50\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: config=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Did you meant to use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     51\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`Sequential.from_config(config)`?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     52\u001b[0m     )\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deserialize\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdeserialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\layers\\serialization.py:249\u001b[0m, in \u001b[0;36mdeserialize\u001b[1;34m(config, custom_objects)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Instantiates a layer from a config dictionary.\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \n\u001b[0;32m    214\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;124;03m```\u001b[39;00m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    248\u001b[0m populate_deserializable_objects()\n\u001b[1;32m--> 249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgeneric_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodule_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLOCAL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mALL_OBJECTS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprintable_module_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlayer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\generic_utils.py:734\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[0;32m    731\u001b[0m custom_objects \u001b[38;5;241m=\u001b[39m custom_objects \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m    733\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom_objects\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m arg_spec\u001b[38;5;241m.\u001b[39margs:\n\u001b[1;32m--> 734\u001b[0m     deserialized_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    735\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcls_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    736\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    737\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_GLOBAL_CUSTOM_OBJECTS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    738\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_THREAD_LOCAL_CUSTOM_OBJECTS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__dict__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    739\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    740\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    741\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    742\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    743\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m CustomObjectScope(custom_objects):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py:3034\u001b[0m, in \u001b[0;36mModel.from_config\u001b[1;34m(cls, config, custom_objects)\u001b[0m\n\u001b[0;32m   3027\u001b[0m functional_model_keys \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   3028\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3029\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayers\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3030\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_layers\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3031\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_layers\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3032\u001b[0m ]\n\u001b[0;32m   3033\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(key \u001b[38;5;129;01min\u001b[39;00m config \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m functional_model_keys):\n\u001b[1;32m-> 3034\u001b[0m     inputs, outputs, layers \u001b[38;5;241m=\u001b[39m \u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreconstruct_from_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3035\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\n\u001b[0;32m   3036\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3037\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\n\u001b[0;32m   3038\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs, outputs\u001b[38;5;241m=\u001b[39moutputs, name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   3039\u001b[0m     )\n\u001b[0;32m   3040\u001b[0m     functional\u001b[38;5;241m.\u001b[39mconnect_ancillary_layers(model, layers)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\functional.py:1469\u001b[0m, in \u001b[0;36mreconstruct_from_config\u001b[1;34m(config, custom_objects, created_layers)\u001b[0m\n\u001b[0;32m   1467\u001b[0m \u001b[38;5;66;03m# First, we create all layers and enqueue nodes to be processed\u001b[39;00m\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_data \u001b[38;5;129;01min\u001b[39;00m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayers\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m-> 1469\u001b[0m     \u001b[43mprocess_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1470\u001b[0m \u001b[38;5;66;03m# Then we process nodes in order of layer depth.\u001b[39;00m\n\u001b[0;32m   1471\u001b[0m \u001b[38;5;66;03m# Nodes that cannot yet be processed (if the inbound node\u001b[39;00m\n\u001b[0;32m   1472\u001b[0m \u001b[38;5;66;03m# does not yet exist) are re-enqueued, and the process\u001b[39;00m\n\u001b[0;32m   1473\u001b[0m \u001b[38;5;66;03m# is repeated until all nodes are processed.\u001b[39;00m\n\u001b[0;32m   1474\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m unprocessed_nodes:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\functional.py:1450\u001b[0m, in \u001b[0;36mreconstruct_from_config.<locals>.process_layer\u001b[1;34m(layer_data)\u001b[0m\n\u001b[0;32m   1446\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1447\u001b[0m     \u001b[38;5;66;03m# Instantiate layer.\u001b[39;00m\n\u001b[0;32m   1448\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deserialize \u001b[38;5;28;01mas\u001b[39;00m deserialize_layer\n\u001b[1;32m-> 1450\u001b[0m     layer \u001b[38;5;241m=\u001b[39m \u001b[43mdeserialize_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1451\u001b[0m     created_layers[layer_name] \u001b[38;5;241m=\u001b[39m layer\n\u001b[0;32m   1453\u001b[0m node_count_by_layer[layer] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(_should_skip_first_node(layer))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\layers\\serialization.py:249\u001b[0m, in \u001b[0;36mdeserialize\u001b[1;34m(config, custom_objects)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Instantiates a layer from a config dictionary.\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \n\u001b[0;32m    214\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;124;03m```\u001b[39;00m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    248\u001b[0m populate_deserializable_objects()\n\u001b[1;32m--> 249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgeneric_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodule_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLOCAL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mALL_OBJECTS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprintable_module_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlayer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\generic_utils.py:744\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[0;32m    742\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    743\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m CustomObjectScope(custom_objects):\n\u001b[1;32m--> 744\u001b[0m             deserialized_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcls_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    745\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    746\u001b[0m     \u001b[38;5;66;03m# Then `cls` may be a function returning a class.\u001b[39;00m\n\u001b[0;32m    747\u001b[0m     \u001b[38;5;66;03m# in this case by convention `config` holds\u001b[39;00m\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;66;03m# the kwargs of the function.\u001b[39;00m\n\u001b[0;32m    749\u001b[0m     custom_objects \u001b[38;5;241m=\u001b[39m custom_objects \u001b[38;5;129;01mor\u001b[39;00m {}\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\layers\\rnn\\lstm.py:893\u001b[0m, in \u001b[0;36mLSTM.from_config\u001b[1;34m(cls, config)\u001b[0m\n\u001b[0;32m    891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimplementation\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config \u001b[38;5;129;01mand\u001b[39;00m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimplementation\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    892\u001b[0m     config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimplementation\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 893\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\layers\\rnn\\lstm.py:584\u001b[0m, in \u001b[0;36mLSTM.__init__\u001b[1;34m(self, units, activation, recurrent_activation, use_bias, kernel_initializer, recurrent_initializer, bias_initializer, unit_forget_bias, kernel_regularizer, recurrent_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, recurrent_constraint, bias_constraint, dropout, recurrent_dropout, return_sequences, return_state, go_backwards, stateful, time_major, unroll, **kwargs)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_spec \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    574\u001b[0m     InputSpec(shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, dim)) \u001b[38;5;28;01mfor\u001b[39;00m dim \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munits, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munits)\n\u001b[0;32m    575\u001b[0m ]\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_could_use_gpu_kernel \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    577\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation \u001b[38;5;129;01min\u001b[39;00m (activations\u001b[38;5;241m.\u001b[39mtanh, tf\u001b[38;5;241m.\u001b[39mtanh)\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurrent_activation \u001b[38;5;129;01min\u001b[39;00m (activations\u001b[38;5;241m.\u001b[39msigmoid, tf\u001b[38;5;241m.\u001b[39msigmoid)\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mexecuting_eagerly_outside_functions()\n\u001b[0;32m    583\u001b[0m )\n\u001b[1;32m--> 584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_logical_devices\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGPU\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;66;03m# Only show the message when there is GPU available, user will not\u001b[39;00m\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;66;03m# care about the cuDNN if there isn't any GPU.\u001b[39;00m\n\u001b[0;32m    587\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_could_use_gpu_kernel:\n\u001b[0;32m    588\u001b[0m         logging\u001b[38;5;241m.\u001b[39mdebug(gru_lstm_utils\u001b[38;5;241m.\u001b[39mCUDNN_AVAILABLE_MSG \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\config.py:459\u001b[0m, in \u001b[0;36mlist_logical_devices\u001b[1;34m(device_type)\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfig.list_logical_devices\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    427\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfig.experimental.list_logical_devices\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    428\u001b[0m \u001b[38;5;129m@deprecation\u001b[39m\u001b[38;5;241m.\u001b[39mdeprecated_endpoints(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfig.experimental.list_logical_devices\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    429\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlist_logical_devices\u001b[39m(device_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    430\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Return a list of logical devices created by runtime.\u001b[39;00m\n\u001b[0;32m    431\u001b[0m \n\u001b[0;32m    432\u001b[0m \u001b[38;5;124;03m  Logical devices may correspond to physical devices or remote devices in the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;124;03m    List of initialized `LogicalDevice`s\u001b[39;00m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 459\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_logical_devices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1579\u001b[0m, in \u001b[0;36mContext.list_logical_devices\u001b[1;34m(self, device_type)\u001b[0m\n\u001b[0;32m   1577\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlist_logical_devices\u001b[39m(\u001b[38;5;28mself\u001b[39m, device_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1578\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Return logical devices.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1579\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1580\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m device_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logical_devices)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:601\u001b[0m, in \u001b[0;36mContext.ensure_initialized\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    599\u001b[0m opts \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_NewContextOptions()\n\u001b[0;32m    600\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 601\u001b[0m   config_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241m.\u001b[39mSerializeToString()\n\u001b[0;32m    602\u001b[0m   pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_ContextOptionsSetConfig(opts, config_str)\n\u001b[0;32m    603\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_device_policy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1099\u001b[0m, in \u001b[0;36mContext.config\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1097\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the ConfigProto with all runtime deltas applied.\"\"\"\u001b[39;00m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# Ensure physical devices have been discovered and config has been imported\u001b[39;00m\n\u001b[1;32m-> 1099\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_physical_devices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1101\u001b[0m config \u001b[38;5;241m=\u001b[39m config_pb2\u001b[38;5;241m.\u001b[39mConfigProto()\n\u001b[0;32m   1102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1449\u001b[0m, in \u001b[0;36mContext._initialize_physical_devices\u001b[1;34m(self, reinitialize)\u001b[0m\n\u001b[0;32m   1446\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1448\u001b[0m devs \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTF_ListPhysicalDevices()\n\u001b[1;32m-> 1449\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_physical_devices \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   1450\u001b[0m     PhysicalDevice(name\u001b[38;5;241m=\u001b[39md\u001b[38;5;241m.\u001b[39mdecode(), device_type\u001b[38;5;241m=\u001b[39md\u001b[38;5;241m.\u001b[39mdecode()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m   1451\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m devs\n\u001b[0;32m   1452\u001b[0m ]\n\u001b[0;32m   1453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_physical_device_to_index \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1454\u001b[0m     p: i \u001b[38;5;28;01mfor\u001b[39;00m i, p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_physical_devices)\n\u001b[0;32m   1455\u001b[0m }\n\u001b[0;32m   1456\u001b[0m \u001b[38;5;66;03m# We maintain a seperate list just so we can check whether the device in\u001b[39;00m\n\u001b[0;32m   1457\u001b[0m \u001b[38;5;66;03m# _physical_devices is a PluggableDevice.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_set_path = r\"F:\\image-caption-generator\\Flickr8k_text\\Flickr_8k.testImages.txt\"\n",
    "test = load_set(test_set_path)\n",
    "print(\"Dataset: %d\" % len(test))\n",
    "\n",
    "# descriptions\n",
    "test_descriptions = load_clean_descriptions(r\"F:\\image-caption-generator\\workspace\\descriptions.txt\", test)\n",
    "print(\"Descriptions: test=%d\" % len(test_descriptions))\n",
    "\n",
    "# photo features\n",
    "test_features = load_photo_features(r\"F:\\image-caption-generator\\workspace\\features.pkl\", test)\n",
    "print('Photos: test=%d' % len(test_features))\n",
    "\n",
    "# load the model which has minimum loss, in this case it was model_18\n",
    "model_name = r\"F:\\image-caption-generator\\workspace\\model\\model_500.h5\"\n",
    "model = keras.models.load_model(model_name)\n",
    "\n",
    "# evalute the model\n",
    "evaluate_model(model, test_descriptions, test_features, tokenizer, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2dd85601-4986-4491-ab2d-f492dd91e08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_read = pickle.load(open(r\"F:\\image-caption-generator\\workspace\\features.pkl\" ,'rb'))\n",
    "import tensorflow as tf\n",
    "model = tf.keras.models.load_model('model/model_500.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20bf8d5e-af07-4876-ad31-68f0277eae4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_prediction(test_path, num_photos_to_predict, test_set_path, test_desc_path, test_features_path):\n",
    "    for i in range(num_photos_to_predict):\n",
    "        rand_image_num = np.random.randint(0, len(os.listdir(test_path)))\n",
    "        image_path = os.path.join(test_path, os.listdir(test_path)[rand_image_num])\n",
    "        image_name = os.path.basename(image_path)\n",
    "        image_name = image_name.split('.')[0]\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        test_set = load_set(test_set_path)\n",
    "        test_descriptions = load_clean_descriptions(test_desc_path, test_set)\n",
    "        test_features = load_photo_features(test_features_path, test_set)\n",
    "        \n",
    "        #captions = descriptions[image_name]\n",
    "        \n",
    "        print('---------------------Actual---------------------')\n",
    "        #for caption in captions:\n",
    "            #print(caption)\n",
    "        \n",
    "        # predict the caption\n",
    "        y_pred = generate_desc(model, tokenizer, feature_read[image_name], max_length)\n",
    "        \n",
    "        print('--------------------Predicted--------------------')\n",
    "\n",
    "        print(y_pred)\n",
    "        plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "012baf90-10d9-4c0e-b560-91a52003f300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------Actual---------------------\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m test_desc_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mimage-caption-generator\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mworkspace\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdescriptions.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m test_features_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mimage-caption-generator\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mworkspace\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mfeatures.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 7\u001b[0m \u001b[43mvisualize_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_set_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_desc_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_features_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[19], line 23\u001b[0m, in \u001b[0;36mvisualize_prediction\u001b[1;34m(test_path, num_photos_to_predict, test_set_path, test_desc_path, test_features_path)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m---------------------Actual---------------------\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m#for caption in captions:\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m#print(caption)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# predict the caption\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m generate_desc(model, tokenizer, \u001b[43mfeature_read\u001b[49m\u001b[43m[\u001b[49m\u001b[43mimage_name\u001b[49m\u001b[43m]\u001b[49m, max_length)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--------------------Predicted--------------------\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(y_pred)\n",
      "\u001b[1;31mKeyError\u001b[0m: '1'"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "test_path = r\"C:\\Users\\srija\\OneDrive\\Desktop\\images\"\n",
    "test_set_path = r\"F:\\image-caption-generator\\Flickr8k_text\\Flickr_8k.trainImages.txt\"\n",
    "test_desc_path = r\"F:\\image-caption-generator\\workspace\\descriptions.txt\"\n",
    "test_features_path = r\"F:\\image-caption-generator\\workspace\\features.pkl\"\n",
    "\n",
    "visualize_prediction(test_path, 1, test_set_path, test_desc_path, test_features_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478ca0eb-5eb5-44b1-bcfd-a94276754334",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
